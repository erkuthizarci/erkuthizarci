{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a76ceb6f",
   "metadata": {},
   "source": [
    "# Extract, Transform, and Load Data using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0e7f51",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Extract, Transform and Load (ETL) operations are of extreme importance in the role of a Data engineer. A data engineer extracts data from multiple sources and different file formats, transforms the extracted data to predefined settings and then loads the data to a database for further processing. In this lab, you will get hands-on practice of performing these operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556fc620",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "After completing this lab, you will be able to:\n",
    "\n",
    "Read CSV, JSON, and XML file types.\n",
    "Extract the required data from the different file types.\n",
    "Transform data to the required format.\n",
    "Save the transformed data in a ready-to-load format, which can be loaded into an RDBMS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6ddf6e",
   "metadata": {},
   "source": [
    "# Gather the data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "257fc0d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZIP file downloaded.\n",
      "ZIP file extracted.\n",
      "Extracted files:\n",
      "['source1.csv', 'source1.json', 'source1.xml', 'source2.csv', 'source2.json', 'source2.xml', 'source3.csv', 'source3.json', 'source3.xml']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "# Step 1: Download the ZIP file\n",
    "\n",
    "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0221EN-SkillsNetwork/labs/module%206/Lab%20-%20Extract%20Transform%20Load/data/source.zip\"\n",
    "zip_file_name = \"source.zip\"\n",
    "\n",
    "# Download the file\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "with open(zip_file_name, 'wb') as file:       # writes the binary content of the downloaded file to a local file source.zip.\n",
    "    file.write(response.content)\n",
    "    \n",
    "print(\"ZIP file downloaded.\")\n",
    "\n",
    "# Step 2: Unzip the file\n",
    "\n",
    "with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:   # Unzipping zip_file_name into new local file source_data\n",
    "    zip_ref.extractall(\"source_data\")\n",
    "    \n",
    "print(\"ZIP file extracted.\")\n",
    "\n",
    "# Step 3: Verify the files\n",
    "\n",
    "print(\"Extracted files:\")\n",
    "\n",
    "print(os.listdir(\"source_data\"))\n",
    "\n",
    "# os.listdir() is a function in the os module which is used to list all the files and directories in a specified directory. \n",
    "# It provides an easy way to inspect the contents of a folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7fbe53",
   "metadata": {},
   "source": [
    "# Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e37ef57",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66994cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "import pandas as pd \n",
    "import xml.etree.ElementTree as ET \n",
    "from datetime import datetime "
   ]
  },
  {
   "cell_type": "raw",
   "id": "84bb24c5",
   "metadata": {},
   "source": [
    "glob: Used for pattern matching and file searching. In this script, it identifies files with specific extensions (*.csv, *.json, *.xml).\n",
    "pandas (as pd): A data manipulation library. It's used to read and process CSV, JSON, and tabular data.\n",
    "xml.etree.ElementTree (as ET): Used to parse and process XML data.\n",
    "datetime: Included for potential timestamping (though not yet used in this code)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2f51eb",
   "metadata": {},
   "source": [
    "Log File and Target File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b76adf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = \"log_file.txt\" \n",
    "target_file = \"transformed_data.csv\" "
   ]
  },
  {
   "cell_type": "raw",
   "id": "ca9089be",
   "metadata": {},
   "source": [
    "log_file: Placeholder for a log file where processing events or errors could be recorded.\n",
    "target_file: Name of the final consolidated CSV file to be created after data processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62334fd3",
   "metadata": {},
   "source": [
    "This script is designed to extract and consolidate data from files in CSV, JSON, and XML formats into a single DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "686201a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_csv(file_to_process): \n",
    "    \n",
    "    dataframe = pd.read_csv(file_to_process) \n",
    "    return dataframe \n",
    "  \n",
    "def extract_from_json(file_to_process):\n",
    "    \n",
    "    dataframe = pd.read_json(file_to_process, lines = True) \n",
    "    return dataframe \n",
    "\n",
    "# The parameter lines = True specifies that the file contains JSON objects on separate lines.\n",
    "  \n",
    "def extract_from_xml(file_to_process): \n",
    "    \n",
    "    dataframe = pd.DataFrame(columns = [\"name\", \"height\", \"weight\"]) \n",
    "    \n",
    "    # Initializes an empty DataFrame with columns name, height, weight.\n",
    "    \n",
    "    tree = ET.parse(file_to_process) \n",
    "    root = tree.getroot() \n",
    "    \n",
    "    # Parses the XML file using ET.parse and retrieves the root element.\n",
    "    \n",
    "    for person in root:  # Iterates through each <person> element in the XML, extracting:\n",
    "        \n",
    "        name = person.find(\"name\").text                        # name (string)\n",
    "        \n",
    "        height = float(person.find(\"height\").text)             # height (converted to float)\n",
    "        \n",
    "        weight = float(person.find(\"weight\").text)             # weight (converted to float)\n",
    "        \n",
    "        dataframe = pd.concat([dataframe, pd.DataFrame([{\"name\":name, \"height\":height, \"weight\":weight}])], ignore_index = True) \n",
    "        \n",
    "        # Appends each person's data as a row to the dataframe.\n",
    "        \n",
    "    return dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce503570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(): \n",
    "    extracted_data = pd.DataFrame(columns = ['name', 'height', 'weight'])  \n",
    "    \n",
    "# create an empty data frame to hold extracted data \n",
    "     \n",
    "    # process all csv files \n",
    "    for csvfile in glob.glob(\"source_data/*.csv\"): \n",
    "        extracted_data = pd.concat([extracted_data, pd.DataFrame(extract_from_csv(csvfile))], ignore_index = True) \n",
    "         \n",
    "    # process all json files \n",
    "    for jsonfile in glob.glob(\"source_data/*.json\"): \n",
    "        extracted_data = pd.concat([extracted_data, pd.DataFrame(extract_from_json(jsonfile))], ignore_index = True) \n",
    "     \n",
    "    # process all xml files \n",
    "    for xmlfile in glob.glob(\"source_data/*.xml\"): \n",
    "        extracted_data = pd.concat([extracted_data, pd.DataFrame(extract_from_xml(xmlfile))], ignore_index = True) \n",
    "         \n",
    "    return extracted_data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd5116f",
   "metadata": {},
   "source": [
    "# Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad59dfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data): \n",
    "    \n",
    "    # Convert inches to meters and round off to two decimals \n",
    "    # 1 inch is 0.0254 meters \n",
    "    \n",
    "    data['height'] = round(data.height * 0.0254, 2) \n",
    "     \n",
    "    # Convert pounds to kilograms and round off to two decimals \n",
    "    # 1 pound is 0.45359237 kilograms \n",
    "    \n",
    "    data['weight'] = round(data.weight * 0.45359237, 2) \n",
    "     \n",
    "    return data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ca106e",
   "metadata": {},
   "source": [
    "# Loading and Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a975356b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(target_file, transformed_data):\n",
    "    transformed_data.to_csv(target_file, index = False)  # Prevent index from being saved\n",
    "\n",
    "    \n",
    "# Saves the transformed data to a CSV file\n",
    "  \n",
    "def log_progress(message): \n",
    "    \n",
    "    timestamp_format = '%Y-%h-%d-%H:%M:%S'     # Year-Monthname-Day-Hour-Minute-Second \n",
    "    \n",
    "    now = datetime.now()                       # get current timestamp \n",
    "    \n",
    "    timestamp = now.strftime(timestamp_format) \n",
    "    \n",
    "    with open(log_file,\"a\") as f: \n",
    "        f.write(timestamp + ',' + message + '\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c62dca0",
   "metadata": {},
   "source": [
    "# Testing ETL operations and log progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27a5e748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Data\n",
      "     name  height  weight\n",
      "0    alex    1.67   51.25\n",
      "1    ajay    1.82   61.91\n",
      "2   alice    1.76   69.41\n",
      "3    ravi    1.73   64.56\n",
      "4     joe    1.72   65.45\n",
      "5    alex    1.67   51.25\n",
      "6    ajay    1.82   61.91\n",
      "7   alice    1.76   69.41\n",
      "8    ravi    1.73   64.56\n",
      "9     joe    1.72   65.45\n",
      "10   alex    1.67   51.25\n",
      "11   ajay    1.82   61.91\n",
      "12  alice    1.76   69.41\n",
      "13   ravi    1.73   64.56\n",
      "14    joe    1.72   65.45\n",
      "15   jack    1.74   55.93\n",
      "16    tom    1.77   64.18\n",
      "17  tracy    1.78   61.90\n",
      "18   john    1.72   50.97\n",
      "19   jack    1.74   55.93\n",
      "20    tom    1.77   64.18\n",
      "21  tracy    1.78   61.90\n",
      "22   john    1.72   50.97\n",
      "23   jack    1.74   55.93\n",
      "24    tom    1.77   64.18\n",
      "25  tracy    1.78   61.90\n",
      "26   john    1.72   50.97\n",
      "27  simon    1.72   50.97\n",
      "28  jacob    1.70   54.73\n",
      "29  cindy    1.69   57.81\n",
      "30   ivan    1.72   51.77\n",
      "31  simon    1.72   50.97\n",
      "32  jacob    1.70   54.73\n",
      "33  cindy    1.69   57.81\n",
      "34   ivan    1.72   51.77\n",
      "35  simon    1.72   50.97\n",
      "36  jacob    1.70   54.73\n",
      "37  cindy    1.69   57.81\n",
      "38   ivan    1.72   51.77\n"
     ]
    }
   ],
   "source": [
    "# Log the initialization of the ETL process \n",
    "log_progress(\"ETL Job Started\") \n",
    "  \n",
    "# Log the beginning of the Extraction process \n",
    "log_progress(\"Extract phase Started\") \n",
    "extracted_data = extract() \n",
    "  \n",
    "# Log the completion of the Extraction process \n",
    "log_progress(\"Extract phase Ended\") \n",
    "  \n",
    "# Log the beginning of the Transformation process \n",
    "log_progress(\"Transform phase Started\") \n",
    "transformed_data = transform(extracted_data) \n",
    "print(\"Transformed Data\") \n",
    "print(transformed_data) \n",
    "  \n",
    "# Log the completion of the Transformation process \n",
    "log_progress(\"Transform phase Ended\") \n",
    "  \n",
    "# Log the beginning of the Loading process \n",
    "log_progress(\"Load phase Started\") \n",
    "load_data(target_file, transformed_data) \n",
    "  \n",
    "# Log the completion of the Loading process \n",
    "log_progress(\"Load phase Ended\") \n",
    "  \n",
    "# Log the completion of the ETL process \n",
    "log_progress(\"ETL Job Ended\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
